{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd9e6f2",
   "metadata": {},
   "source": [
    "## Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d32c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad4d94",
   "metadata": {},
   "source": [
    "BDM Corpus 2\n",
    "\n",
    "Bom Dia Mercado (BDM) → xlsx file with BDM articles and more → preprocessing to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f47c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "news_df = pd.read_excel(\"../data/raw/bdm-corpus-2.xlsx\")\n",
    "\n",
    "# Normalize individual DATE and TIME cells\n",
    "def parse_datetime_components(date_cell, time_cell):\n",
    "    try:\n",
    "        # Coerce both to string and strip spaces\n",
    "        date_str = str(date_cell).strip()\n",
    "        time_str = str(time_cell).strip()\n",
    "        \n",
    "        # Combine and parse flexibly\n",
    "        dt = parser.parse(f\"{date_str} {time_str}\", dayfirst=True)\n",
    "        return dt.isoformat()\n",
    "    except Exception:\n",
    "        return pd.NaT  # mark invalid rows\n",
    "\n",
    "# Create ISO 8601 Timestamp column\n",
    "news_df['Timestamp'] = news_df.apply(lambda row: parse_datetime_components(row['DATE'], row['TIME']), axis=1)\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Drop old columns\n",
    "news_df.drop(columns=['DATE', 'TIME', 'Index', 'DIRECTION', 'BRER', 'LABEL'], inplace=True)\n",
    "\n",
    "# Clean newlines in ARTICLE CONTENT and COMMENTS\n",
    "for col in ['HEADING', 'ARTICLE CONTENT', 'COMMENTS']:\n",
    "    if col in news_df.columns:\n",
    "        news_df[col] = news_df[col].astype(str).str.replace(r'[\\r\\n]+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Reorder columns\n",
    "news_df = news_df[['Timestamp'] + [col for col in news_df.columns if col != 'Timestamp']]\n",
    "\n",
    "# Rename \"HEADING\" to \"Headline\" \"ARTICLE CONTENT\" to \"Article\" and \"COMMENTS\" to \"Comments\"\n",
    "news_df.rename(columns={\n",
    "    'HEADING': 'Headline',\n",
    "    'ARTICLE CONTENT': 'Article',\n",
    "    'COMMENTS': 'Comments'\n",
    "}, inplace=True)\n",
    "\n",
    "# save\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506b239",
   "metadata": {},
   "source": [
    "### Check for invalid rows (rows with no headlines) and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830aa006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 invalid rows found in 'Headline' column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2024-12-16 09:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>2025-01-10 12:18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>2025-01-10 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>2025-01-10 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp Headline Article Comments\n",
       "2332  2024-12-16 09:02:00      NaN     NaN      NaN\n",
       "4299  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4302  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4306  2025-01-10 12:18:00      NaN     NaN      NaN\n",
       "4367  2025-01-10 15:58:00      NaN     NaN      NaN\n",
       "4372  2025-01-10 15:59:00      NaN     NaN      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", encoding='utf-8-sig') # reload as csv to ensure correct encoding\n",
    "\n",
    "invalid_rows = news_df[news_df['Headline'].isna()]\n",
    "print(f\"{len(invalid_rows)} invalid rows found in 'Headline' column.\")\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with invalid headlines and resave\n",
    "news_df = news_df.dropna(subset=['Headline'])\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a43be",
   "metadata": {},
   "source": [
    "## Exchange Rate Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c9b77",
   "metadata": {},
   "source": [
    "Bloomberg → Download USD/BRL exchange rates as excel file → preprocess to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ae1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load \n",
    "df_usd_brl = pd.read_excel(\"../data/raw/usd-brl.xlsx\")\n",
    "\n",
    "# Step 1: Clean column names\n",
    "df_usd_brl.columns = [col.strip() for col in df_usd_brl.columns]\n",
    "df_usd_brl.rename(columns={\"Date\": \"Raw Timestamp\", \"Último preço\": \"USD/BRL\"}, inplace=True)\n",
    "\n",
    "# Step 2: Parse \"Raw Timestamp\" directly into pandas datetime (no ISO string conversion)\n",
    "df_usd_brl[\"Timestamp\"] = pd.to_datetime(df_usd_brl[\"Raw Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Step 3: Drop the original column\n",
    "df_usd_brl.drop(columns=[\"Raw Timestamp\"], inplace=True)\n",
    "\n",
    "# Step 4: Reorder columns\n",
    "df_usd_brl = df_usd_brl[[\"Timestamp\", \"USD/BRL\"]]\n",
    "\n",
    "# Step 5: Save\n",
    "df_usd_brl.to_csv(\"../data/interim/usd-brl.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new datasets\n",
    "bdm_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", parse_dates=['Timestamp'])\n",
    "fx_df = pd.read_csv(\"../data/interim/usd-brl.csv\", parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df615adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5929 duplicate timestamps.\n",
      "Removed duplicate timestamps.\n",
      "Set 'Timestamp' as index and sorted chronologically.\n",
      "Forward-filled missing minute-level timestamps.\n",
      "Timestamps are now continuous and minute-by-minute. No gaps remain.\n"
     ]
    }
   ],
   "source": [
    "# check how many duplicate timestamps there are\n",
    "num_duplicates = fx_df.duplicated(subset=\"Timestamp\").sum()\n",
    "print(f\"Found {num_duplicates} duplicate timestamps.\")\n",
    "\n",
    "# remove duplicate timestamps (keep first occurrence)\n",
    "fx_df = fx_df.drop_duplicates(subset=\"Timestamp\", keep=\"first\")\n",
    "print(\"Removed duplicate timestamps.\")\n",
    "\n",
    "# set timestamp index and sort\n",
    "fx_df = fx_df.set_index(\"Timestamp\").sort_index()\n",
    "print(\"Set 'Timestamp' as index and sorted chronologically.\")\n",
    "\n",
    "# forward fill missing timestamps to create continuous minute-level series\n",
    "fx_df = fx_df.resample(\"1min\").ffill()\n",
    "print(\"Forward-filled missing minute-level timestamps.\")\n",
    "\n",
    "# verify that the dataframe is now fully continuous\n",
    "expected_index = pd.date_range(start=fx_df.index.min(), end=fx_df.index.max(), freq=\"1min\")\n",
    "missing_timestamps = expected_index.difference(fx_df.index)\n",
    "\n",
    "if missing_timestamps.empty:\n",
    "    print(\"Timestamps are now continuous and minute-by-minute. No gaps remain.\")\n",
    "else:\n",
    "    print(f\"{len(missing_timestamps)} missing timestamps still remain:\")\n",
    "    print(missing_timestamps[:10])  # preview first 10 missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "fx_df.to_csv(\"../data/interim/usd-brl-continuous.csv\", index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e1bbb",
   "metadata": {},
   "source": [
    "## Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959af05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  - remove all rows following the last timestamp in the interim/bdm-corpus-2/stage-0.csv at 2024-12-30 17:32:00\n",
    "  - remove article and comments columns from interim/bdm-corpus-2/stage-0.csv\n",
    "  - merge the USD/BRL values from interim/usd-brl-continuous.csv into interim/bdm-corpus-2/stage-0.csv by matching timestamps\n",
    "  - compute forward returns based on the prediction horizon t+1 to t+20 minutes\n",
    "    - positive returns will map to +1, negative returns will map to -1, and no change will map to 0\n",
    "    - each computed forward return will be stored in a new column named \"Forward Return t+X\" where X is the number of minutes ahead\n",
    "  - the function will return a new DataFrame with the merged data and forward returns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged dataset: 3519\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "fx_df = pd.read_csv(\"../data/interim/usd-brl-continuous.csv\", parse_dates=[\"Timestamp\"])\n",
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", parse_dates=[\"Timestamp\"])\n",
    "\n",
    "# Restrict news to timestamps before cutoff\n",
    "last_timestamp = pd.to_datetime(\"2024-12-30 17:32:00\")\n",
    "news_df = news_df[news_df[\"Timestamp\"] <= last_timestamp]\n",
    "\n",
    "# Keep only timestamp and headline\n",
    "news_df = news_df[[\"Timestamp\", \"Headline\"]]\n",
    "\n",
    "# --- Compute forward returns within fx_df ---\n",
    "def compute_forward_returns(df, horizon_minutes=20):\n",
    "    df = df.copy()\n",
    "    for i in range(1, horizon_minutes + 1):\n",
    "        col = f\"Forward Return t+{i}\"\n",
    "        delta = df[\"USD/BRL\"].shift(-i) - df[\"USD/BRL\"]\n",
    "        df[col] = np.sign(delta)  # Maps to -1, 0, or 1\n",
    "    return df\n",
    "\n",
    "fx_df = compute_forward_returns(fx_df)\n",
    "\n",
    "# --- Merge the forward returns into news_df ---\n",
    "merged_df = pd.merge(news_df, fx_df, on=\"Timestamp\", how=\"left\")\n",
    "\n",
    "# Report row count\n",
    "print(f\"Number of rows in merged dataset: {len(merged_df)}\")\n",
    "\n",
    "# Save as stage-1 dataset\n",
    "merged_df.to_csv(\"../data/interim/bdm-corpus-2/stage-1.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f95d0f",
   "metadata": {},
   "source": [
    "## Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a1f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stage 2 Dataset Creation Process:\n",
    "- rename t+5 forward return to ground truth\n",
    "- remove all t+X forward returns EXCEPT t+5\n",
    "- remove usd/brl column, as it's not needed for this since the ground truth delta is already calculated\n",
    "\n",
    "Intraday Trading Sessions: Remove all rows with headlines released outside of market hours (mostly weekends in our data)\n",
    "    - Mention it's intraday news in the prompt\n",
    "    - Intraday: news from 5 minutes before market open till 5 minutes before market close\n",
    "    - For news at end of trading day, the price change is determined by the last trading price (not always t+5), very few instances of this\n",
    "    - 54 headlines (rows) were removed as they were either neutral or took place on weekends\n",
    "    - I'd rather keep this a binary classification task and let the \"hold\" be determined by the trader based on profits or num of trades\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809eacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-1.csv\", parse_dates=['Timestamp'])\n",
    "prices = pd.read_csv(\"../data/interim/usd-brl.csv\", parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80de578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter by TRADING DAYS only ---\n",
    "valid_dates = set(prices[\"Timestamp\"].dt.date.unique())\n",
    "\n",
    "def is_trading_day(ts):\n",
    "    return ts.date() in valid_dates\n",
    "\n",
    "df = df[df[\"Timestamp\"].apply(is_trading_day)]\n",
    "\n",
    "# --- Drop USD/BRL column if it exists ---\n",
    "df.drop(columns=[\"USD/BRL\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "# --- Extract and rename t+5 forward return column ---\n",
    "t5_col = [col for col in df.columns if \"t+5\" in col and \"forward return\" in col.lower()]\n",
    "if t5_col:\n",
    "    df = df[[\"Timestamp\", \"Headline\", t5_col[0]]]\n",
    "    df.rename(columns={t5_col[0]: \"Direction\"}, inplace=True)\n",
    "\n",
    "    # --- Map numeric values to direction labels ---\n",
    "    df[\"Direction\"] = df[\"Direction\"].replace({1.0: \"Increase\", -1.0: \"Decrease\", 0.0: \"Stable\"})\n",
    "\n",
    "    # --- Drop rows where direction is Stable ---\n",
    "    df = df[df[\"Direction\"] != \"Stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rename columns and translate labels to Brazilian Portuguese ---\n",
    "df.rename(columns={\n",
    "    \"Timestamp\": \"DataHora\",\n",
    "    \"Headline\": \"Manchete\",\n",
    "    \"Direction\": \"Direção\"\n",
    "}, inplace=True)\n",
    "\n",
    "df[\"Direção\"] = df[\"Direção\"].replace({\n",
    "    \"Increase\": \"Aumento\",\n",
    "    \"Decrease\": \"Diminuição\"\n",
    "})\n",
    "\n",
    "df.to_csv(\"../data/interim/bdm-corpus-2/stage-2.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a3a70",
   "metadata": {},
   "source": [
    "## Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- LLaMA-3 preprocessing (keep text natural to preserve syntactic value)\n",
    "- DO NOT remove stopwords\n",
    "- DO NOT lemmatize or stem\n",
    "- DO NOT lowercase\n",
    "- DO NOT translate or normalize to English\n",
    "- Preserve accents, diacritics, and original formatting\n",
    "    \n",
    "Reason: LLaMA-3 is a base model trained on diverse, natural language\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a17544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define robust headline cleaner ===\n",
    "def clean_headline(text: str) -> str:\n",
    "    text = str(text).strip()\n",
    "\n",
    "    # Normalize smart quotes and apostrophes\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "\n",
    "    # Remove repeated quotes/apostrophes\n",
    "    text = re.sub(r'\"{2,}', '\"', text)\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # Remove leading/trailing quotes (even if multiple)\n",
    "    text = re.sub(r'^([\"\\']+)', '', text)\n",
    "    text = re.sub(r'([\"\\']+)$', '', text)\n",
    "\n",
    "    # Remove noisy special character sequences\n",
    "    text = re.sub(r\"[_•√×+÷=<>^~|#*@¬]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"[_•√×+÷=<>^~|#*@¬]\", \"\", text)\n",
    "\n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove non-printable/unusual characters except Latin-1\n",
    "    text = re.sub(r\"[^\\x20-\\x7EÀ-ÿ°€¢£¥‰–—…]\", \" \", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Manchete\"] = df[\"Manchete\"].apply(clean_headline)\n",
    "df.to_csv(\"../data/processed/bdm-corpus-2/stage-3/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fadafb",
   "metadata": {},
   "source": [
    "### Create train/test sets out of Stage 3 data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/bdm-corpus-2/stage-3/data.csv\")  # Adjust path if needed\n",
    "\n",
    "# Filter by 'Direção'\n",
    "aumento = df[df[\"Direção\"] == \"Aumento\"]\n",
    "diminuicao = df[df[\"Direção\"] == \"Diminuição\"]\n",
    "\n",
    "# Sample 100 rows from each category\n",
    "sample_aumento = aumento.sample(n=200, random_state=42) # reproducible with seed set, same set generated every time\n",
    "sample_diminuicao = diminuicao.sample(n=200, random_state=42) # same as above\n",
    "\n",
    "# Combine and shuffle the test set\n",
    "test_df = pd.concat([sample_aumento, sample_diminuicao])\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Remove test samples from the original DataFrame\n",
    "to_remove_indices = sample_aumento.index.union(sample_diminuicao.index)\n",
    "train_df = df.drop(index=to_remove_indices).reset_index(drop=True)\n",
    "\n",
    "# Save to new CSVs\n",
    "test_df.to_csv(\"../data/processed/bdm-corpus-2/stage-3/test.csv\", index=False)\n",
    "train_df.to_csv(\"../data/processed/bdm-corpus-2/stage-3/train.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
