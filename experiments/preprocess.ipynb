{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd9e6f2",
   "metadata": {},
   "source": [
    "## Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d32c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad4d94",
   "metadata": {},
   "source": [
    "BDM Corpus 2\n",
    "\n",
    "Bom Dia Mercado (BDM) → xlsx file with BDM articles and more → preprocessing to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f47c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "news_df = pd.read_excel(\"../data/raw/bdm-corpus-2.xlsx\")\n",
    "\n",
    "# Normalize individual DATE and TIME cells\n",
    "def parse_datetime_components(date_cell, time_cell):\n",
    "    try:\n",
    "        # Coerce both to string and strip spaces\n",
    "        date_str = str(date_cell).strip()\n",
    "        time_str = str(time_cell).strip()\n",
    "        \n",
    "        # Combine and parse flexibly\n",
    "        dt = parser.parse(f\"{date_str} {time_str}\", dayfirst=True)\n",
    "        return dt.isoformat()\n",
    "    except Exception:\n",
    "        return pd.NaT  # mark invalid rows\n",
    "\n",
    "# Create ISO 8601 Timestamp column\n",
    "news_df['Timestamp'] = news_df.apply(lambda row: parse_datetime_components(row['DATE'], row['TIME']), axis=1)\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Drop old columns\n",
    "news_df.drop(columns=['DATE', 'TIME', 'Index', 'DIRECTION', 'BRER', 'LABEL'], inplace=True)\n",
    "\n",
    "# Clean newlines in ARTICLE CONTENT and COMMENTS\n",
    "for col in ['HEADING', 'ARTICLE CONTENT', 'COMMENTS']:\n",
    "    if col in news_df.columns:\n",
    "        news_df[col] = news_df[col].astype(str).str.replace(r'[\\r\\n]+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Reorder columns\n",
    "news_df = news_df[['Timestamp'] + [col for col in news_df.columns if col != 'Timestamp']]\n",
    "\n",
    "# Rename \"HEADING\" to \"Headline\" \"ARTICLE CONTENT\" to \"Article\" and \"COMMENTS\" to \"Comments\"\n",
    "news_df.rename(columns={\n",
    "    'HEADING': 'Headline',\n",
    "    'ARTICLE CONTENT': 'Article',\n",
    "    'COMMENTS': 'Comments'\n",
    "}, inplace=True)\n",
    "\n",
    "# save\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506b239",
   "metadata": {},
   "source": [
    "### Check for invalid rows (rows with no headlines) and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830aa006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 invalid rows found in 'Headline' column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2024-12-16 09:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>2025-01-10 12:18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>2025-01-10 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>2025-01-10 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp Headline Article Comments\n",
       "2332  2024-12-16 09:02:00      NaN     NaN      NaN\n",
       "4299  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4302  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4306  2025-01-10 12:18:00      NaN     NaN      NaN\n",
       "4367  2025-01-10 15:58:00      NaN     NaN      NaN\n",
       "4372  2025-01-10 15:59:00      NaN     NaN      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", encoding='utf-8-sig') # reload as csv to ensure correct encoding\n",
    "\n",
    "invalid_rows = news_df[news_df['Headline'].isna()]\n",
    "print(f\"{len(invalid_rows)} invalid rows found in 'Headline' column.\")\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed35e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with invalid headlines and resave\n",
    "news_df = news_df.dropna(subset=['Headline'])\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a43be",
   "metadata": {},
   "source": [
    "## Exchange Rate Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c9b77",
   "metadata": {},
   "source": [
    "Bloomberg → Download USD/BRL exchange rates as excel file → preprocess to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2ae1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load \n",
    "df_usd_brl = pd.read_excel(\"../data/raw/usd-brl.xlsx\")\n",
    "\n",
    "# Step 1: Clean column names\n",
    "df_usd_brl.columns = [col.strip() for col in df_usd_brl.columns]\n",
    "df_usd_brl.rename(columns={\"Date\": \"Raw Timestamp\", \"Último preço\": \"USD/BRL\"}, inplace=True)\n",
    "\n",
    "# Step 2: Parse \"Raw Timestamp\" directly into pandas datetime (no ISO string conversion)\n",
    "df_usd_brl[\"Timestamp\"] = pd.to_datetime(df_usd_brl[\"Raw Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Step 3: Drop the original column\n",
    "df_usd_brl.drop(columns=[\"Raw Timestamp\"], inplace=True)\n",
    "\n",
    "# Step 4: Reorder columns\n",
    "df_usd_brl = df_usd_brl[[\"Timestamp\", \"USD/BRL\"]]\n",
    "\n",
    "# Step 5: Save\n",
    "df_usd_brl.to_csv(\"../data/interim/usd-brl.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74442fb",
   "metadata": {},
   "source": [
    "Clean up the interim stage of the exchange rate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f4f066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate timestamps found:\n",
      "                Timestamp  USD/BRL\n",
      "1617  2024-09-05 09:00:00   5.6409\n",
      "1618  2024-09-05 09:00:00   5.6409\n",
      "1619  2024-09-05 09:01:00   5.6437\n",
      "1620  2024-09-05 09:01:00   5.6437\n",
      "1621  2024-09-05 09:02:00   5.6413\n",
      "...                   ...      ...\n",
      "35023 2024-11-14 17:56:00   5.7945\n",
      "35024 2024-11-14 17:57:00   5.7958\n",
      "35025 2024-11-14 17:57:00   5.7958\n",
      "35026 2024-11-14 17:58:00   5.7964\n",
      "35027 2024-11-14 17:58:00   5.7964\n",
      "\n",
      "[11858 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/interim/usd-brl.csv\")\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df = df.sort_values('Timestamp').reset_index(drop=True) # Sort to ensure chronological order\n",
    "\n",
    "# Check for duplicate timestamps\n",
    "duplicate_rows = df[df.duplicated(subset=['Timestamp'], keep=False)]\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate timestamps found:\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate timestamps found.\")\n",
    "\n",
    "# Remove duplicates (keeping first occurrence)\n",
    "df = df.drop_duplicates(subset=['Timestamp'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687af30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intraday gaps found:\n",
      "\n",
      "Date: 2024-12-24\n",
      "  09:14:00 → 09:16:00  (2 min gap)\n",
      "  09:16:00 → 09:23:00  (7 min gap)\n",
      "  09:23:00 → 09:27:00  (4 min gap)\n",
      "  09:28:00 → 09:53:00  (25 min gap)\n",
      "  09:54:00 → 09:57:00  (3 min gap)\n",
      "  09:57:00 → 09:59:00  (2 min gap)\n",
      "  09:59:00 → 10:02:00  (3 min gap)\n",
      "  10:03:00 → 10:06:00  (3 min gap)\n",
      "  10:07:00 → 10:32:00  (25 min gap)\n",
      "\n",
      "Removed all rows from days with intraday gaps: {datetime.date(2024, 12, 24)}\n"
     ]
    }
   ],
   "source": [
    "# Detect intraday gaps\n",
    "df['TimeDiff'] = df['Timestamp'].diff()\n",
    "\n",
    "intraday_gaps = df[\n",
    "    (df['TimeDiff'] > pd.Timedelta(minutes=1)) &\n",
    "    (df['Timestamp'].dt.date == df['Timestamp'].shift().dt.date)\n",
    "]\n",
    "\n",
    "# Group gaps by date\n",
    "intraday_gap_summary = {}\n",
    "for idx, row in intraday_gaps.iterrows():\n",
    "    prev_time = df.loc[idx - 1, 'Timestamp']\n",
    "    curr_time = row['Timestamp']\n",
    "    gap_minutes = int(row['TimeDiff'].total_seconds() // 60)\n",
    "    date = curr_time.date()\n",
    "    intraday_gap_summary.setdefault(date, []).append(\n",
    "        (prev_time.time(), curr_time.time(), gap_minutes)\n",
    "    )\n",
    "\n",
    "# Display results\n",
    "if intraday_gap_summary:\n",
    "    print(\"\\nIntraday gaps found:\")\n",
    "    for date, gaps in intraday_gap_summary.items():\n",
    "        print(f\"\\nDate: {date}\")\n",
    "        for prev_t, curr_t, gap in gaps:\n",
    "            print(f\"  {prev_t} → {curr_t}  ({gap} min gap)\")\n",
    "else:\n",
    "    print(\"\\nNo intraday gaps found.\")\n",
    "\n",
    "# Remove days with intraday gaps\n",
    "if intraday_gap_summary:\n",
    "    gap_dates = set(intraday_gap_summary.keys())\n",
    "    df = df[~df['Timestamp'].dt.date.isin(gap_dates)].reset_index(drop=True)\n",
    "    print(f\"\\nRemoved all rows from days with intraday gaps: {gap_dates}\")\n",
    "\n",
    "# Drop helper column\n",
    "df = df.drop(columns=['TimeDiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfdd6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/usd-brl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e1bbb",
   "metadata": {},
   "source": [
    "## Stage 1\n",
    "  - remove all rows following the last timestamp in the interim/bdm-corpus-2/stage-0.csv at 2024-12-30 17:32:00\n",
    "  - remove article and comments columns from interim/bdm-corpus-2/stage-0.csv\n",
    "  - merge the USD/BRL values from interim/usd-brl-continuous.csv into interim/bdm-corpus-2/stage-0.csv by matching timestamps\n",
    "  - compute forward returns based on the price from t+1 to t+20 minutes subtracted from the price during the timestamp of the news\n",
    "    - positive returns will map to +1, negative returns will map to -1, and no change will map to 0\n",
    "    - each computed forward return will be stored in a new column named \"Forward Return t+X\" where X is the number of minutes ahead\n",
    "\n",
    "    - incorporate a stability threshold when mapping \"no change\":\n",
    "      - unchanged is defined as an absolute log return less than or equal to the Nth percentile of all 1-minute absolute log returns in the dataset (default: 60th percentile)\n",
    "      - this replaces the exact-zero check for unchanged, allowing small price movements to be considered stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9206e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total news rows before cutoff: 3523\n",
      "News timestamps matching FX bars: 3506\n",
      "Invalid Forward Return t+1 (NaN): 20\n",
      "Invalid Forward Return t+5 (NaN): 25\n",
      "Invalid Forward Return t+10 (NaN): 35\n",
      "Invalid Forward Return t+20 (NaN): 108\n"
     ]
    }
   ],
   "source": [
    "# INCLUDE stable metric\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "fx_df = pd.read_csv(\"../data/processed/usd-brl.csv\", parse_dates=[\"Timestamp\"])\n",
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-0.csv\", parse_dates=[\"Timestamp\"])\n",
    "\n",
    "# Restrict news to timestamps before cutoff\n",
    "last_timestamp = pd.to_datetime(\"2024-12-30 17:58:00\")\n",
    "news_df = news_df[news_df[\"Timestamp\"] < last_timestamp]\n",
    "news_df = news_df[[\"Timestamp\", \"Headline\"]]\n",
    "\n",
    "'''\n",
    "Compute forward returns with day-end invalidation\n",
    "Remove 2nd parameter and argue 3rd param to False for no band\n",
    "'''\n",
    "def compute_forward_returns(df, horizon_minutes=20, stable_quantile=0.60, use_stable_band=True):\n",
    "    df = df.copy()\n",
    "\n",
    "    if use_stable_band:\n",
    "        r1 = np.log(df[\"USD/BRL\"]).diff().abs()\n",
    "        thr = r1.quantile(stable_quantile)\n",
    "    else:\n",
    "        thr = 0  # unchanged means exactly zero change\n",
    "\n",
    "    df[\"Date\"] = df[\"Timestamp\"].dt.date\n",
    "    day_end = df.groupby(\"Date\")[\"Timestamp\"].transform(\"max\")\n",
    "    for i in range(1, horizon_minutes + 1):\n",
    "        col = f\"Forward Return t+{i}\"\n",
    "        ret_i = np.log(df[\"USD/BRL\"].shift(-i)) - np.log(df[\"USD/BRL\"])\n",
    "        s = np.sign(ret_i)\n",
    "        s = np.where(ret_i.abs() <= thr, 0, s)\n",
    "        s = pd.Series(s, index=df.index).where(df[\"Timestamp\"] + pd.Timedelta(minutes=i) <= day_end, np.nan)\n",
    "        df[col] = s\n",
    "    return df.drop(columns=\"Date\")\n",
    "\n",
    "fx_df = compute_forward_returns(fx_df, stable_quantile=0.60, use_stable_band=True)\n",
    "\n",
    "# Merge into news_df\n",
    "merged_df = pd.merge(news_df, fx_df, on=\"Timestamp\", how=\"left\")\n",
    "\n",
    "# Key checks\n",
    "print(f\"Total news rows before cutoff: {len(news_df)}\")\n",
    "print(f\"News timestamps matching FX bars: {merged_df['USD/BRL'].notna().sum()}\")\n",
    "for i in (1, 5, 10, 20):\n",
    "    col = f\"Forward Return t+{i}\"\n",
    "    invalid_count = merged_df[col].isna().sum()\n",
    "    print(f\"Invalid {col} (NaN): {invalid_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2949d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\n",
    "    \"../data/interim/bdm-corpus-2/stage-1.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\",\n",
    "    na_rep=\"NA\" # will be used later to identify which forward return horizons we can't use when assigning ground truth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f95d0f",
   "metadata": {},
   "source": [
    "## Stage 2\n",
    "- assign t+5 minutes forward return as ground truth (Direção) - map from +1/-1 to increase/decrease\n",
    "- remove all t+X forward returns EXCEPT t+5\n",
    "- remove usd/brl column, as it's not needed for this since the ground truth delta is already calculated\n",
    "\n",
    "Intraday Trading Sessions: Remove all non-intraday data, includes any news outside of market hours (weekends/holidays)\n",
    "- Data now contains news from 5 minutes before market open till 5 minutes before market close\n",
    "- For news at end of trading day, the price change is determined by the last trading price (not always t+5), very few instances of this\n",
    "\n",
    "Binary Classification Task: Remove any rows that contain no change in price **(forward return == 0)** beteen timstamp t and t+5.\n",
    "- 54 headlines (rows) were removed as they were either neutral (news on weekends also removed)\n",
    "- Better as a binary classification task and let \"hold asset\" be determined by the trader based on profits or num of trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80de578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-1.csv\", na_values=[\"NA\"])\n",
    "\n",
    "def make_horizon_df(df, horizon):\n",
    "    \"\"\"\n",
    "    Create a filtered DataFrame for a specific forward return horizon.\n",
    "    Keeps only Timestamp, Headline, and mapped Direction.\n",
    "    Reusable function\n",
    "    \"\"\"\n",
    "    col_name = f\"Forward Return t+{horizon}\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{col_name}' not found in DataFrame.\")\n",
    "\n",
    "    out = df[[\"Timestamp\", \"Headline\", col_name]].copy()\n",
    "    out.rename(columns={col_name: \"Direction\"}, inplace=True)\n",
    "\n",
    "    # Map numeric to labels\n",
    "    mapping = {1.0: \"Increase\", -1.0: \"Decrease\", 0.0: \"Stable\"}\n",
    "    out[\"Direction\"] = out[\"Direction\"].map(mapping)\n",
    "\n",
    "    # Drop non-intraday / invalid forward return rows\n",
    "    out = out.dropna(subset=[\"Direction\"])\n",
    "\n",
    "    return out\n",
    "\n",
    "# Usage (t+5 for now)\n",
    "t5_df = make_horizon_df(merged_df, 5)\n",
    "t5_df.to_csv(\"../data/interim/bdm-corpus-2/stage-2.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a3a70",
   "metadata": {},
   "source": [
    "## Stage 3\n",
    "Preparing for language model inference\n",
    "- DO NOT remove stopwords\n",
    "- DO NOT lemmatize or stem\n",
    "- DO NOT lowercase\n",
    "- DO NOT translate or normalize to English\n",
    "- Preserve accents, diacritics, and original formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "033e2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a17544fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define robust headline cleaner ===\n",
    "def clean_headline(text: str) -> str:\n",
    "    text = str(text).strip()\n",
    "\n",
    "    # Normalize smart quotes and apostrophes\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "\n",
    "    # Remove repeated quotes/apostrophes\n",
    "    text = re.sub(r'\"{2,}', '\"', text)\n",
    "    text = re.sub(r\"'{2,}\", \"'\", text)\n",
    "\n",
    "    # Remove leading/trailing quotes (even if multiple)\n",
    "    text = re.sub(r'^([\"\\']+)', '', text)\n",
    "    text = re.sub(r'([\"\\']+)$', '', text)\n",
    "\n",
    "    # Remove noisy special character sequences\n",
    "    text = re.sub(r\"[_•√×+÷=<>^~|#*@¬]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"[_•√×+÷=<>^~|#*@¬]\", \"\", text)\n",
    "\n",
    "    # Normalize unicode\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # Remove non-printable/unusual characters except Latin-1\n",
    "    text = re.sub(r\"[^\\x20-\\x7EÀ-ÿ°€¢£¥‰–—…]\", \" \", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8be5731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Headline\"] = df[\"Headline\"].apply(clean_headline)\n",
    "df.to_csv(\"../data/interim/bdm-corpus-2/stage-3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3d9a1a",
   "metadata": {},
   "source": [
    "## Stage 4\n",
    "- remove duplicate rows (eg., row_a timestamp/headline == row_b timestamp/headline)\n",
    "    - note: some timestamps contain multiple headlines. They are all different, no need to fret over this\n",
    "- removed all rows with duplicate \"Manchete\" column value and kept first occurence\n",
    "- chronologically create train/test set for temporal evaluation\n",
    "- keep test set slightly imbalanced, as it already is, to mimic real world scenario\n",
    "- 3465 headlines -> 3457 headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a42659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before removing duplicates: 3498\n",
      "Duplicate rows:\n",
      "                 Timestamp                                           Headline  \\\n",
      "238   2024-11-26 10:02:00                                  Reação ao IPCA-15   \n",
      "241   2024-11-26 10:02:00                                  Reação ao IPCA-15   \n",
      "488   2024-11-28 09:02:00  AOVIVO/Haddad sobre IR: Nosso objetivo é que e...   \n",
      "489   2024-11-28 09:02:00  Ela irá beneficiar todo mundo que ganha até 5 ...   \n",
      "490   2024-11-28 09:02:00  Com essa fórmula de cálculo, a suposta renúnci...   \n",
      "493   2024-11-28 09:02:00  AOVIVO/Haddad sobre IR: Nosso objetivo é que e...   \n",
      "494   2024-11-28 09:02:00  Ela irá beneficiar todo mundo que ganha até 5 ...   \n",
      "495   2024-11-28 09:02:00  Com essa fórmula de cálculo, a suposta renúnci...   \n",
      "599   2024-11-28 11:54:00  MERCADOS: Sob pressão do fiscal, Ibovespa amea...   \n",
      "600   2024-11-28 11:54:00  MERCADOS: Sob pressão do fiscal, Ibovespa amea...   \n",
      "851   2024-12-02 10:56:00  AOVIVO/Galípolo diz que a questão da meta é pá...   \n",
      "852   2024-12-02 10:56:00  AOVIVO/Galípolo diz que a questão da meta é pá...   \n",
      "1440  2024-12-06 09:22:00  MERCADOS: dolar e juros sobem com fiscal no ra...   \n",
      "1441  2024-12-06 09:22:00  MERCADOS: dolar e juros sobem com fiscal no ra...   \n",
      "1516  2024-12-06 12:54:00  Fed/Goolsbee: Inflação cai, mas se estabilizar...   \n",
      "1517  2024-12-06 12:54:00  Fed/Goolsbee: Inflação cai, mas se estabilizar...   \n",
      "\n",
      "     Direction  \n",
      "238   Increase  \n",
      "241   Increase  \n",
      "488   Decrease  \n",
      "489   Decrease  \n",
      "490   Decrease  \n",
      "493   Decrease  \n",
      "494   Decrease  \n",
      "495   Decrease  \n",
      "599   Increase  \n",
      "600   Increase  \n",
      "851   Decrease  \n",
      "852   Decrease  \n",
      "1440  Decrease  \n",
      "1441  Decrease  \n",
      "1516  Decrease  \n",
      "1517  Decrease  \n",
      "Duplicate Headline count: 180\n",
      "\n",
      "Total rows after removing duplicates: 3343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from babel.dates import format_datetime\n",
    "\n",
    "df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-3.csv\")\n",
    "print(\"Total rows before removing duplicates:\", len(df))\n",
    "\n",
    "# Drop exact (Timestamp, Headline) duplicates\n",
    "dupes = df[df.duplicated(subset=[\"Timestamp\", \"Headline\"], keep=False)]\n",
    "print(\"Duplicate rows:\\n\", dupes)\n",
    "df = df.drop_duplicates(subset=[\"Timestamp\", \"Headline\"], keep=\"first\")\n",
    "\n",
    "# Drop headline duplicates (model shouldn't be trained on multiple instances of a headline, for semantic reasons)\n",
    "dupe_heads = df[df.duplicated(subset=\"Headline\", keep=False)]\n",
    "print(\"Duplicate Headline count:\", len(dupe_heads))\n",
    "df = df.drop_duplicates(subset=\"Headline\", keep=\"first\")\n",
    "\n",
    "print(f\"\\nTotal rows after removing duplicates: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/interim/bdm-corpus-2/stage-4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69e374",
   "metadata": {},
   "source": [
    "## Stage 5 (Temporary) - filtering out noisy headlines\n",
    "REMOVED:\n",
    "- Removed via code:\n",
    "    - Anything that starts with \"Reação\" since lacks context and articles are too noisy\n",
    "- Manually removed: (check row numbers in stage-4), re-do everytime when running preprocessing from beginning... should change that:\n",
    "    - Row 3129: 2024-12-26 13:29:00, \"Inicialmente, dado seria publicado à tarde\"\n",
    "    - Row 574: 2024-11-28 11:25:00, \"Mensagem apagada\"\n",
    "    - Row 587: 2024-11-28 12:44:00, \"Mídia oculta\"\n",
    "\n",
    "Important terms:\n",
    "- losses: perda, queda, recuo\n",
    "- fiscal, abaixo (fell under), Galípolo (prez of central bank of BR)\n",
    "\n",
    "Terms to Ignore: Ibovespa, Stoxx600, Dow, S&P, Nasdaq (stock market terms)\n",
    "\n",
    "Change labels to: em alta/baixa or valorização/desvalorização\n",
    "\n",
    "Create with datasets with neutral and without (filtered) for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "738697c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Timestamp                                           Headline  \\\n",
      "46    2024-11-22 09:00:00                    Reação aos PMIs da Zona do euro   \n",
      "159   2024-11-25 12:36:00                      Reação a Conta Corrente e IDP   \n",
      "231   2024-11-26 10:00:00                                  Reação ao IPCA-15   \n",
      "444   2024-11-27 16:07:00                                    Reação ao Caged   \n",
      "537   2024-11-28 10:44:00                            Reação ao pacote fiscal   \n",
      "543   2024-11-28 10:57:00                    Reação ao pacote/Bradesco Asset   \n",
      "577   2024-11-28 12:08:00                                   Reação ao pacote   \n",
      "641   2024-11-29 10:01:00                                      Reação à Pnad   \n",
      "745   2024-11-29 16:12:00                            Reação à bandeira verde   \n",
      "785   2024-12-02 10:25:00                      Reação ao PMI da zona do euro   \n",
      "894   2024-12-03 09:37:00                             Reação ao PIB do 3oTri   \n",
      "904   2024-12-03 09:47:00                                      Reação ao PIB   \n",
      "1388  2024-12-06 10:41:00                                  Reação ao Payroll   \n",
      "1632  2024-12-10 09:41:00                         Reação ao IPCA de novembro   \n",
      "1776  2024-12-11 10:37:00                                      Reação ao CPI   \n",
      "1816  2024-12-11 12:55:00                              Reação ao CPI dos EUA   \n",
      "1966  2024-12-12 10:53:00                                      Reação ao BCE   \n",
      "1995  2024-12-12 11:33:00                                   Reação ao Varejo   \n",
      "2290  2024-12-16 16:39:00  Reação ao Relatório do Tesouro de projeções fi...   \n",
      "2636  2024-12-18 17:44:00                                      Reação ao Fed   \n",
      "3119  2024-12-26 12:34:00   Reação aos pedidos de auxílio-desemprego dos EUA   \n",
      "3259  2024-12-30 10:22:00                          Reação ao Déficit Público   \n",
      "\n",
      "     Direction  \n",
      "46    Decrease  \n",
      "159   Increase  \n",
      "231   Increase  \n",
      "444     Stable  \n",
      "537     Stable  \n",
      "543   Increase  \n",
      "577   Decrease  \n",
      "641   Decrease  \n",
      "745   Decrease  \n",
      "785     Stable  \n",
      "894   Decrease  \n",
      "904   Increase  \n",
      "1388    Stable  \n",
      "1632  Decrease  \n",
      "1776  Increase  \n",
      "1816    Stable  \n",
      "1966  Increase  \n",
      "1995  Increase  \n",
      "2290  Increase  \n",
      "2636  Decrease  \n",
      "3119    Stable  \n",
      "3259  Decrease  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/interim/bdm-corpus-2/stage-4.csv\")\n",
    "\n",
    "removed = df[df['Headline'].str.startswith(\"Reação\", na=False)]\n",
    "df = df[~df['Headline'].str.startswith(\"Reação\", na=False)]\n",
    "print(removed)\n",
    "\n",
    "df.to_csv(\"../data/processed/bdm-corpus-2/stage-5/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91ef53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/processed/bdm-corpus-2/stage-5/data.csv\")\n",
    "\n",
    "# Original\n",
    "train_original = df.iloc[:-500]\n",
    "test_original = df.tail(500)\n",
    "\n",
    "# Filtered (remove stable first, for equal sets)\n",
    "df_filtered = df[df['Direction'] != 'Stable']\n",
    "train_filtered = df_filtered.iloc[:-500]\n",
    "test_filtered = df_filtered.tail(500)\n",
    "\n",
    "# Save\n",
    "train_original.to_csv(\"../data/processed/bdm-corpus-2/stage-5/train_original.csv\", index=False)\n",
    "test_original.to_csv(\"../data/processed/bdm-corpus-2/stage-5/test_original.csv\", index=False)\n",
    "train_filtered.to_csv(\"../data/processed/bdm-corpus-2/stage-5/train_filtered.csv\", index=False)\n",
    "test_filtered.to_csv(\"../data/processed/bdm-corpus-2/stage-5/test_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3572add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train (Original)\n",
      "Direction\n",
      "Increase    1096\n",
      "Decrease    1046\n",
      "Stable       676\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test (Original)\n",
      "Direction\n",
      "Increase    214\n",
      "Decrease    155\n",
      "Stable      131\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train (Filtered)\n",
      "Direction\n",
      "Increase    1045\n",
      "Decrease     966\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test (Filtered)\n",
      "Direction\n",
      "Increase    265\n",
      "Decrease    235\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path(\"../data/processed/bdm-corpus-2/stage-5\")\n",
    "\n",
    "datasets = {\n",
    "    \"Train (Original)\": pd.read_csv(base_path / \"train_original.csv\"),\n",
    "    \"Test (Original)\": pd.read_csv(base_path / \"test_original.csv\"),\n",
    "    \"Train (Filtered)\": pd.read_csv(base_path / \"train_filtered.csv\"),\n",
    "    \"Test (Filtered)\": pd.read_csv(base_path / \"test_filtered.csv\")\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(df['Direction'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
