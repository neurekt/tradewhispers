{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official Preprocessing Code for PT-BR Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/labeled_january_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_jan = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/processed/labeled_february_data.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df_feb = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# spacy PT model\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "#preprocessing\n",
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # lemmatization and stopwords removal\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    #tokens back to 1 string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# preprocess ALL articles in january's dataframe\n",
    "df_jan['preprocessed_article'] = df_jan['article'].apply(preprocess_text_spacy)\n",
    "df_feb['preprocessed_article'] = df_feb['article'].apply(preprocess_text_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing January BDM Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_file(inputFile, outputFile):\n",
    "#     with open(inputFile, 'r', encoding='utf-8') as file:\n",
    "#         cleaned_lines = []\n",
    "        \n",
    "#         for line in file:\n",
    "            \n",
    "#             cleaned_line = line.replace('[', '').replace(']', '').replace('…', '').strip()\n",
    "            \n",
    "#             if cleaned_line and cleaned_line[2] == '/' and cleaned_line[5] == '/':\n",
    "#                 if cleaned_lines: \n",
    "#                     cleaned_lines.append('')  \n",
    "\n",
    "#             if cleaned_line:\n",
    "#                 cleaned_lines.append(cleaned_line)\n",
    "\n",
    "\n",
    "#     with open(outputFile, 'w', encoding='utf-8') as file:\n",
    "#         for line in cleaned_lines:\n",
    "#             file.write(line + '\\n')\n",
    "\n",
    "# clean_file('data/news_corpus.txt', 'data/news_corpus_cleaned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing article file data into a DataFrame for readability and easier to convert to CSV\n",
    "- PLEASE SAVE THIS OUTPUTTED DATA AS A CSV\n",
    "- this is what i manually did for january, not exactly reproducable, be careful with this, i built the dataframe for february differently\n",
    "- i need to get every data source into this dataframe format because it's easy to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINSIHED PREPROCESSING JANUARY DATA, SAVED AS labeled_january_data.csv now, keeping this code for reusability later on\n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from datetime import datetime\n",
    "\n",
    "# # look at the file and make it into a list\n",
    "# def parse_articles_to_df(file_path):\n",
    "#     dates = []  # keep the dates\n",
    "#     articles = []  # keep the articles\n",
    "    \n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         current_date = None  # keep the date we are looking at right now\n",
    "#         current_articles = []  # keep the article for the current date\n",
    "        \n",
    "#         for line in file:\n",
    "#             line = line.strip()  # take away spaces from the start and end\n",
    "#             if line:  # if the line is not empty\n",
    "#                 if line[2] == \"/\":  # if the line looks like a date (dd/mm/yy)\n",
    "#                     # we had a date before, so let's save it with its articles\n",
    "#                     if current_date:\n",
    "#                         for article in current_articles:\n",
    "#                             dates.append(current_date)  # add current date\n",
    "#                             articles.append(article)  # add article for that date\n",
    "#                     # use the date as is, don't change it\n",
    "#                     current_date = line  # keep new date\n",
    "#                     current_articles = []  # start fresh for new date\n",
    "#                 else:\n",
    "#                     # 'line' is the article, keep adding them to the list\n",
    "#                     current_articles.append(line) \n",
    "        \n",
    "#         if current_date:  # when we are done looking at the file\n",
    "#             for article in current_articles:  # for all the articles we saved\n",
    "#                 dates.append(current_date)  # add the date again\n",
    "#                 articles.append(article)  # add the article again\n",
    "    \n",
    "#     df = pd.DataFrame({'date': dates, 'article': articles})  # make table with the dates and articles\n",
    "#     return df  # display table\n",
    "\n",
    "# file_path = \"data/news_corpus_cleaned.txt\"  # where the file is\n",
    "# df_template = parse_articles_to_df(file_path)  # call function to make the table, called df_template because it can be used for another month\n",
    "\n",
    "# # show the table to see it\n",
    "# display(df_template.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what dates are in our file filled with articles\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_dates_from_file(file_path):\n",
    "    date_pattern = r'\\d{2}/\\d{2}/\\d{2}'\n",
    "    dates = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        matches = re.findall(date_pattern, content)\n",
    "        dates.extend(matches)\n",
    "    \n",
    "    dates_df = pd.DataFrame(dates, columns=['Date'])\n",
    "    return dates_df\n",
    "\n",
    "file_path = 'data/news_corpus_cleaned.txt'\n",
    "dates_df = extract_dates_from_file(file_path)\n",
    "display(dates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing February BDM Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# excel_file_path = \"data/raw/bdm_march_clean_sort_of.xlsx\"  \n",
    "# output_directory = \"data/processed/march_2024_csv_files\"  \n",
    "\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "# excel_data = pd.ExcelFile(excel_file_path)  # load Excel file\n",
    "\n",
    "# for sheet_name in excel_data.sheet_names:\n",
    "#     # load current sheet into a DataFrame\n",
    "#     df_sheet = excel_data.parse(sheet_name)\n",
    "    \n",
    "#     # construct output CSV file path\n",
    "#     output_csv_path = os.path.join(output_directory, f\"{sheet_name}.csv\")\n",
    "    \n",
    "#     df_sheet.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "#     print(f\"Saved {sheet_name} to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for NA or inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = \"data/february_2024_csv_files/Sheet1.csv\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "print(\"NA Values: \", df[\"Label\"].isna().any())\n",
    "print(\"Infinity values: \", np.isinf(df[\"Label\"]).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows = df[df[\"Label\"].isna() | np.isinf(df[\"Label\"])]\n",
    "print(invalid_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all NA labels in every sheet to 0 and convert them to ints instead of floats\n",
    "- good for reproducibility, probably will implement into a script that can be reused to clean and label incoming articles\n",
    "\n",
    "- DO NOT RUN THIS CODE AS IT WILL CORRUPT THE FILES, it is raw, manual preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = \"data/february_2024_csv_files\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        df[\"Label\"] = df[\"Label\"].astype(int)\n",
    "        df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith(\".csv\"):\n",
    "#         filepath = os.path.join(directory, filename)\n",
    "#         df = pd.read_csv(filepath)\n",
    "#         df.rename(columns={df.columns[1]: 'article'}, inplace=True)\n",
    "\n",
    "#         df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… Mesmo reconhecendo que há uma desinflação si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… Em uma 3ªF bastante movimentada, Haddad conv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… O ministro disse que o governo deverá enviar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… Isso significa que haveria um prazo de 45 di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… Segundo Haddad, a sugestão para que o tema d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>HYPERA. Assembleia aprovou aumento de capital ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>TENDA informou que foi liquidada a operação re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>… Montante total líquido recebido pela companh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>AERIS registrou prejuízo líquido de R$ 63,853 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>22/02/2024</td>\n",
       "      <td>LIGHT disse que segue em tratativas com princi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                            article  label\n",
       "0   22/02/2024  … Mesmo reconhecendo que há uma desinflação si...      0\n",
       "1   22/02/2024  … Em uma 3ªF bastante movimentada, Haddad conv...      0\n",
       "2   22/02/2024  … O ministro disse que o governo deverá enviar...      1\n",
       "3   22/02/2024  … Isso significa que haveria um prazo de 45 di...      1\n",
       "4   22/02/2024  … Segundo Haddad, a sugestão para que o tema d...      1\n",
       "..         ...                                                ...    ...\n",
       "81  22/02/2024  HYPERA. Assembleia aprovou aumento de capital ...      0\n",
       "82  22/02/2024  TENDA informou que foi liquidada a operação re...      0\n",
       "83  22/02/2024  … Montante total líquido recebido pela companh...      0\n",
       "84  22/02/2024  AERIS registrou prejuízo líquido de R$ 63,853 ...      0\n",
       "85  22/02/2024  LIGHT disse que segue em tratativas com princi...      0\n",
       "\n",
       "[86 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #sample of each sheet (can be used prior to preprocessing to check for missing values)\n",
    "\n",
    "# file_path = \"data/february_2024_csv_files/Sheet1.csv\"\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#     df = pd.read_csv(file)\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the problematic CSV\n",
    "\n",
    "# file = \"data/february_2024_csv_files/Sheet4.csv\"\n",
    "# df = pd.read_csv(file)\n",
    "\n",
    "# # Fix the date column format\n",
    "# df['date'] = pd.to_datetime(df['date']).dt.strftime('%m/%d/%y')\n",
    "\n",
    "# # Save the updated CSV\n",
    "# df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# all_data = []\n",
    "\n",
    "# # sort filenames based on the numeric part of the filename (Sheet1, Sheet2, etc)\n",
    "# filenames = sorted([filename for filename in os.listdir(directory) if filename.endswith(\".csv\")],\n",
    "#                    key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "# for filename in filenames:\n",
    "#     filepath = os.path.join(directory, filename)\n",
    "#     df = pd.read_csv(filepath)\n",
    "#     all_data.append(df)\n",
    "\n",
    "# labeled_february_data_df = pd.concat(all_data, ignore_index=True)\n",
    "# labeled_february_data_df.to_csv(\"labeled_february_data.csv\", index=False)\n",
    "\n",
    "# display(labeled_february_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange CSVs with {date} and label columns to have date, article, and label columns and handles missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "directory = \"data/processed/march_news_corpus_cleaned\"\n",
    "dataframes = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        date = df.columns[0]\n",
    "        df.rename(columns={date: 'article'}, inplace=True)\n",
    "        df.insert(loc=0, column='date', value=date)\n",
    "        dataframes.append(df)\n",
    "\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "final_df['label'] = final_df['label'].fillna(0)\n",
    "final_df['label'] = final_df['label'].astype(int)\n",
    "final_df.to_csv('data/processed/labeled_march_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging files script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Eli\n",
    "\n",
    "import os\n",
    "\n",
    "#check working directory \n",
    "os.getcwd() \n",
    "\n",
    "directory = os.listdir('data/News_Sample/andre')\n",
    "\n",
    "for item in range(len(directory)):\n",
    "    directory[item] = r'data/News_Sample/andre/' + directory[item]\n",
    "\n",
    "#check for correct file names\n",
    "print(directory)\n",
    "\n",
    "#merge all files\n",
    "with open('data/merged_files.txt', 'w') as outfile:\n",
    "    for file in directory:\n",
    "        fhand = open(file, 'r', encoding='utf-8')\n",
    "        fname = fhand.read()\n",
    "        outfile.write(fname)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Corpus Preprocessing for Lexicon Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
