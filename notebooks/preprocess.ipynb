{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd9e6f2",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d32c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320e67b",
   "metadata": {},
   "source": [
    "## News Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad4d94",
   "metadata": {},
   "source": [
    "Bom Dia Mercado (BDM) → xlsx file with BDM articles and more → preprocessing to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f47c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "news_df = pd.read_excel(\"../data/raw/bdm-corpus-2.xlsx\")\n",
    "\n",
    "# Normalize individual DATE and TIME cells\n",
    "def parse_datetime_components(date_cell, time_cell):\n",
    "    try:\n",
    "        # Coerce both to string and strip spaces\n",
    "        date_str = str(date_cell).strip()\n",
    "        time_str = str(time_cell).strip()\n",
    "        \n",
    "        # Combine and parse flexibly\n",
    "        dt = parser.parse(f\"{date_str} {time_str}\", dayfirst=True)\n",
    "        return dt.isoformat()\n",
    "    except Exception:\n",
    "        return pd.NaT  # mark invalid rows\n",
    "\n",
    "# Create ISO 8601 Timestamp column\n",
    "news_df['Timestamp'] = news_df.apply(lambda row: parse_datetime_components(row['DATE'], row['TIME']), axis=1)\n",
    "news_df['Timestamp'] = pd.to_datetime(news_df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Drop old columns\n",
    "news_df.drop(columns=['DATE', 'TIME', 'Index', 'DIRECTION', 'BRER', 'LABEL'], inplace=True)\n",
    "\n",
    "# Clean newlines in ARTICLE CONTENT and COMMENTS\n",
    "for col in ['HEADING', 'ARTICLE CONTENT', 'COMMENTS']:\n",
    "    if col in news_df.columns:\n",
    "        news_df[col] = news_df[col].astype(str).str.replace(r'[\\r\\n]+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Reorder columns\n",
    "news_df = news_df[['Timestamp'] + [col for col in news_df.columns if col != 'Timestamp']]\n",
    "\n",
    "# Rename \"HEADING\" to \"Headline\" \"ARTICLE CONTENT\" to \"Article\" and \"COMMENTS\" to \"Comments\"\n",
    "news_df.rename(columns={\n",
    "    'HEADING': 'Headline',\n",
    "    'ARTICLE CONTENT': 'Article',\n",
    "    'COMMENTS': 'Comments'\n",
    "}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3506b239",
   "metadata": {},
   "source": [
    "### Check for invalid rows (rows with no headlines) and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830aa006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 invalid rows found in 'Headline' column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Article</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2024-12-16 09:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>2025-01-10 12:14:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>2025-01-10 12:18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>2025-01-10 15:58:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>2025-01-10 15:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp Headline Article Comments\n",
       "2332  2024-12-16 09:02:00      NaN     NaN      NaN\n",
       "4299  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4302  2025-01-10 12:14:00      NaN     NaN      NaN\n",
       "4306  2025-01-10 12:18:00      NaN     NaN      NaN\n",
       "4367  2025-01-10 15:58:00      NaN     NaN      NaN\n",
       "4372  2025-01-10 15:59:00      NaN     NaN      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2.csv\", encoding='utf-8-sig') # reload as csv to ensure correct encoding\n",
    "\n",
    "invalid_rows = news_df[news_df['Headline'].isna()]\n",
    "print(f\"{len(invalid_rows)} invalid rows found in 'Headline' column.\")\n",
    "display(invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed35e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with invalid headlines and resave\n",
    "news_df = news_df.dropna(subset=['Headline'])\n",
    "news_df.to_csv(\"../data/interim/bdm-corpus-2.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a43be",
   "metadata": {},
   "source": [
    "## Exchange Rate Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c9b77",
   "metadata": {},
   "source": [
    "Bloomberg → Download USD/BRL exchange rates as excel file → preprocess to CSV → export to repository → final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2ae1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Load the dataset\n",
    "df_usd_brl = pd.read_excel(\"../data/raw/usd-brl.xlsx\")\n",
    "\n",
    "# Step 1: Clean column names\n",
    "df_usd_brl.columns = [col.strip() for col in df_usd_brl.columns]\n",
    "df_usd_brl.rename(columns={\"Date\": \"Raw Timestamp\", \"Último preço\": \"USD/BRL\"}, inplace=True)\n",
    "\n",
    "# Step 2: Parse \"Raw Timestamp\" directly into pandas datetime (no ISO string conversion)\n",
    "df_usd_brl[\"Timestamp\"] = pd.to_datetime(df_usd_brl[\"Raw Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Step 3: Drop the original column\n",
    "df_usd_brl.drop(columns=[\"Raw Timestamp\"], inplace=True)\n",
    "\n",
    "# Step 4: Reorder columns\n",
    "df_usd_brl = df_usd_brl[[\"Timestamp\", \"USD/BRL\"]]\n",
    "\n",
    "# Step 5: Save to CSV\n",
    "df_usd_brl.to_csv(\"../data/interim/usd-brl.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3083ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new datasets\n",
    "bdm_df = pd.read_csv(\"../data/interim/bdm-corpus-2.csv\", parse_dates=['Timestamp'])\n",
    "fx_df = pd.read_csv(\"../data/interim/usd-brl.csv\", parse_dates=['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df615adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5929 duplicate timestamps.\n",
      "Removed duplicate timestamps.\n",
      "Set 'Timestamp' as index and sorted chronologically.\n",
      "Forward-filled missing minute-level timestamps.\n",
      "Timestamps are now continuous and minute-by-minute. No gaps remain.\n"
     ]
    }
   ],
   "source": [
    "# check how many duplicate timestamps there are\n",
    "num_duplicates = fx_df.duplicated(subset=\"Timestamp\").sum()\n",
    "print(f\"Found {num_duplicates} duplicate timestamps.\")\n",
    "\n",
    "# remove duplicate timestamps (keep first occurrence)\n",
    "fx_df = fx_df.drop_duplicates(subset=\"Timestamp\", keep=\"first\")\n",
    "print(\"Removed duplicate timestamps.\")\n",
    "\n",
    "# set timestamp index and sort\n",
    "fx_df = fx_df.set_index(\"Timestamp\").sort_index()\n",
    "print(\"Set 'Timestamp' as index and sorted chronologically.\")\n",
    "\n",
    "# forward fill missing timestamps to create continuous minute-level series\n",
    "fx_df = fx_df.resample(\"1min\").ffill()\n",
    "print(\"Forward-filled missing minute-level timestamps.\")\n",
    "\n",
    "# verify that the dataframe is now fully continuous\n",
    "expected_index = pd.date_range(start=fx_df.index.min(), end=fx_df.index.max(), freq=\"1min\")\n",
    "missing_timestamps = expected_index.difference(fx_df.index)\n",
    "\n",
    "if missing_timestamps.empty:\n",
    "    print(\"Timestamps are now continuous and minute-by-minute. No gaps remain.\")\n",
    "else:\n",
    "    print(f\"{len(missing_timestamps)} missing timestamps still remain:\")\n",
    "    print(missing_timestamps[:10])  # preview first 10 missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "fx_df.to_csv(\"../data/interim/usd-brl-continuous.csv\", index=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e1bbb",
   "metadata": {},
   "source": [
    "## Final Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959af05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions needed for experimental dataset creation:\n",
    "- Abilities:\n",
    "  - dataset with +1, -1 for binary forward returns\n",
    "  - dataset with +1, -1, and 0 by forward return threshold of choice\n",
    "  - dataset with forward returns as percentage changes\n",
    "  - dataset with forward returns as absolute changes\n",
    "  - dataset with only timestamps and headlines\n",
    "  - dataset with only timestamps, headlines, and articles\n",
    "  - load in original datasets\n",
    "  - choose prediction horizon by choice of minutes (eg. t+1, t+5, t+15, etc.)\n",
    "  - map forward returns +1, -1 for decrease to measure directional accuracy\n",
    "  - map forward returns +1, -1, 0 for thresholded returns to measure directional accuracy\n",
    "\n",
    "I will develop functionality that will prepare a single experimental dataset:\n",
    "  - remove all rows following the last timestamp in the interim/brl-corpus-2.csv at 2024-12-30 17:32:00\n",
    "  - remove article and comments columns from interim/brl-corpus-2.csv\n",
    "  - merge the USD/BRL values from interim/usd-brl-continuous.csv into interim/brl-corpus-2.csv by matching timestamps\n",
    "  - compute forward returns based on the prediction horizon t+1 to t+20 minutes\n",
    "    - positive returns will map to +1, negative returns will map to -1, and no change will map to 0\n",
    "    - each computed forward return will be stored in a new column named \"Forward Return t+X\" where X is the number of minutes ahead\n",
    "  - the function will return a new DataFrame with the merged data and forward returns\n",
    "  - save the new DataFrame to a CSV file named \"experimental_dataset.csv\"\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a04b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in merged dataset: 3519\n"
     ]
    }
   ],
   "source": [
    "fx_df = pd.read_csv(\"../data/interim/usd-brl-continuous.csv\", parse_dates=['Timestamp'])\n",
    "news_df = pd.read_csv(\"../data/interim/bdm-corpus-2.csv\", parse_dates=['Timestamp'])\n",
    "\n",
    "# remove all rows following the last timestamp in the interim/brl-corpus-2.csv at 2024-12-30 17:32:00\n",
    "last_timestamp = pd.to_datetime(\"2024-12-30 17:32:00\")\n",
    "news_df = news_df[news_df['Timestamp'] <= last_timestamp]\n",
    "\n",
    "# remove article and comments columns from interim/brl-corpus-2.csv\n",
    "news_df = news_df[['Timestamp', 'Headline']]\n",
    "\n",
    "# Merge the datasets on Timestamp\n",
    "merged_df = pd.merge(news_df, fx_df, on='Timestamp', how='left')\n",
    "\n",
    "# count rows in merged_df\n",
    "num_rows = len(merged_df)\n",
    "print(f\"Number of rows in merged dataset: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4bcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "compute forward returns based on the prediction horizon t+1 to t+20 minutes\n",
    "    - positive returns will map to +1, negative returns will map to -1, and no change will map to 0\n",
    "'''\n",
    "\n",
    "def compute_forward_returns(df, horizon_minutes=20):\n",
    "    for i in range(1, horizon_minutes + 1):\n",
    "        # new columns for each forward return \n",
    "        col_name = f'Forward Return t+{i}'\n",
    "        df[col_name] = np.nan\n",
    "        \n",
    "        # calculate the forward return\n",
    "        df[col_name] = df['USD/BRL'].shift(-i) - df['USD/BRL'] #this calculates the forward return by shifting the USD/BRL column by i minutes and subtracting the current value\n",
    "        \n",
    "        # map to +1, -1, 0 for directional accuracy (DA) metric that i'll use later on\n",
    "        df[col_name] = np.where(df[col_name] > 0, 1, np.where(df[col_name] < 0, -1, 0))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683c3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_forward_returns(merged_df, horizon_minutes=20)\n",
    "merged_df.to_csv(\"../data/processed/experimental_dataset.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
